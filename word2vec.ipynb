{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rainer\\Desktop\\github\\word2vec\\.venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "['i', 'love', 'mulch', 'mulch', 'gang', 'for', 'life', 'i', 'love', 'ingesting', 'microplastic']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I love mulch mulch gang for life i love ingesting microplastic\"\n",
    "tokens = list(sentence.lower().split())\n",
    "print(len(tokens))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we create a dictionary object that maps vocab to integer idices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, 'i': 1, 'love': 2, 'mulch': 3, 'gang': 4, 'for': 5, 'life': 6, 'ingesting': 7, 'microplastic': 8}\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "\n",
    "vocab[\"<pad>\"] = 0\n",
    "\n",
    "index = 1\n",
    "for token in tokens: \n",
    "    if token not in vocab:\n",
    "        vocab[token] = index\n",
    "        index +=1\n",
    "\n",
    "print(vocab)\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and an inverse dictionary index -> word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<pad>', 1: 'i', 2: 'love', 3: 'mulch', 4: 'gang', 5: 'for', 6: 'life', 7: 'ingesting', 8: 'microplastic'}\n"
     ]
    }
   ],
   "source": [
    "inverse_vocab = {}\n",
    "for token, index in vocab.items():\n",
    "    inverse_vocab[index] = token\n",
    "\n",
    "print(inverse_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vectorize sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 3, 4, 5, 6, 1, 2, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "example_sequence = [vocab[word] for word in tokens]\n",
    "print(example_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generating skip gram word pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "(6, 1): (life, i)\n",
      "(5, 6): (for, life)\n",
      "(4, 5): (gang, for)\n",
      "(2, 8): (love, microplastic)\n",
      "(3, 4): (mulch, gang)\n"
     ]
    }
   ],
   "source": [
    "window_size = 2\n",
    "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "      example_sequence,\n",
    "      vocabulary_size=vocab_size,\n",
    "      window_size=window_size,\n",
    "      negative_samples=0)\n",
    "print(len(positive_skip_grams))\n",
    "\n",
    "for target, context in positive_skip_grams[:5]:\n",
    "  print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generating negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([3 1 5 2 4], shape=(5,), dtype=int64)\n",
      "['mulch', 'i', 'for', 'love', 'gang']\n",
      "tf.Tensor([[1]], shape=(1, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "target_word, context_word = positive_skip_grams[0]\n",
    "num_ns = 5\n",
    "\n",
    "\n",
    "context_class = tf.reshape(tf.constant(context_word, dtype = \"int64\"), (1,1))\n",
    "\n",
    "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "    true_classes=context_class,  # class that should be sampled as 'positive'\n",
    "    num_true=1,  # each positive skip-gram has 1 positive context class\n",
    "    num_sampled=num_ns,  # number of negative context words to sample\n",
    "    unique=True,  # all the negative samples should be unique\n",
    "    range_max=vocab_size,  # pick index of the samples from [0, vocab_size]\n",
    "    seed=SEED,  # seed for reproducibility\n",
    "    name=\"negative_sampling\"  # name of this operation\n",
    ")\n",
    "print(negative_sampling_candidates)\n",
    "print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])\n",
    "print(context_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## constructing a training ex\n",
    "concat one context word w negative samples and label them.1 - context, 0 - ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "squeezed_context_class = tf.squeeze(context_class, 1)\n",
    "context = tf.concat([squeezed_context_class, negative_sampling_candidates], 0) #one positive ex, num_ns negative ex concated \n",
    "label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "target = target_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compiling a function that constructs training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "    targets, contexts, labels = [],[],[]\n",
    "\n",
    "    for sequence in tqdm.tqdm(sequences):\n",
    "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "        example_sequence,\n",
    "        vocabulary_size=vocab_size,\n",
    "        window_size=window_size,\n",
    "        negative_samples=0)\n",
    "\n",
    "        for target_word, context_word in positive_skip_grams:\n",
    "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "            true_classes=context_class,  # class that should be sampled as 'positive'\n",
    "            num_true=1,  # each positive skip-gram has 1 positive context class\n",
    "            num_sampled=num_ns,  # number of negative context words to sample\n",
    "            unique=True,  # all the negative samples should be unique\n",
    "            range_max=vocab_size,  # pick index of the samples from [0, vocab_size]\n",
    "            seed=SEED,  # seed for reproducibility\n",
    "            name=\"negative_sampling\"  # name of this operation\n",
    "            )\n",
    "\n",
    "            squeezed_context_class = tf.squeeze(context_class, 1)\n",
    "            context = tf.concat([squeezed_context_class, negative_sampling_candidates], 0) #one positive ex, num_ns negative ex concated \n",
    "            label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "            target = target_word\n",
    "\n",
    "            targets.append(target)\n",
    "            contexts.append(context)\n",
    "            labels.append(label)\n",
    "\n",
    "    return targets, contexts, labels\n",
    "\n",
    "            \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt') \n",
    "#fetch file from uyrl and return local file path\n",
    "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))\n",
    "#parse file into a TextLineDataset object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing\n",
    "standardize vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\rainer\\Desktop\\github\\word2vec\\.venv\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\rainer\\Desktop\\github\\word2vec\\.venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  return tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')  \n",
    "\n",
    "vocab_size = 4096\n",
    "sequence_length = 10\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "  standardize = custom_standardization,\n",
    "  max_tokens = vocab_size,\n",
    "  output_mode = \"int\",\n",
    "  output_sequence_length = sequence_length\n",
    ")\n",
    "\n",
    "vectorize_layer.adapt(text_ds.batch(1024))\n",
    "\n",
    "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this returns. a tf.data.Dataset object containing vectoried sequences. \n",
    "now we flatten into a list of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32777\n"
     ]
    }
   ],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate training sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32777 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32777/32777 [19:49<00:00, 27.55it/s] \n"
     ]
    }
   ],
   "source": [
    "targets, contexts, labels = generate_training_data(sequences, 5, 4, vocab_size, SEED)\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ds = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "training_ds = training_ds.cache().prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.target_embedding = layers.Embedding(\n",
    "            vocab_size = vocab_size, embedding_dim = embedding_dim, input_length = 1, name = \"target layer\" \n",
    "        )\n",
    "        self.context_embedding = layers.Embedding(\n",
    "            vocab_size = vocab_size, embedding_dim = embedding_dim, input_length = num_ns+1, name = \"context layer\" \n",
    "        )\n",
    "\n",
    "    def call(self, pair):\n",
    "        target, context = pair\n",
    "\n",
    "        if len(target.shape) == 2:\n",
    "            target = tf.squeeze(target, 1)\n",
    "\n",
    "        target_emb = self.target_embedding(target)\n",
    "        context_emb = self.context_embedding(context)\n",
    "\n",
    "        dots = tf.einsum('be,bce->bc', target_emb, context_emb)\n",
    "        return dots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(vocab_size, 128)\n",
    "word2vec.compile(optimizer=\"adam\", loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=\"accuracy\")\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "\n",
    "word2vec.fit(training_ds, epoch = 5, callback = [tensorboard_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
